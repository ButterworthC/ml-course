<html>
<head>
<title>e-Portfolio for ML course 2024</title>
</head>

<body>
<h1>e-Portfolio for ML course 2024</h1>
<h2>Chris Butterworth</h2>
<p>
<a href="about-me.html">About me</a><br>
<a href="prep.html">Preparation for Machine Learning module</a><br>
</p>
<p>
The notebooks and data files are available at <a href="https://github.com/ButterworthC/ml-course">https://github.com/ButterworthC/ml-course</a>
</p>

<h3>Unit 1: Introduction to Machine Learning (ML)</h3>
<p>
In this unit we were introduced to Schwab &amp; Zahid's paper on the impact of machine learning on the workplace and future levels of employment.
The lecturecast described the effect of technology in bringing about <i>Industry 4.0</i>,
a concept described as <i>cyber-physical systems</i> (<a href="#schwab">Schwab &amp; Zahid, 2020</a>).
Supervised and unsupervised machine learning were mentioned briefly, as were the 4 V's of big data (Volume, Velocity, Variety and Veracity).
</p>

<h3>Unit 2: Exploratory Data Analysis</h3>
<p>Much of this unit involved preparation for the collaborative discussion on Industry 4.0, the upcoming team data analysis project, and the e-Portfolio.
We were directed to a notebook which demonstrates EDA. Unfortunately I was unable to find this in Google Colab so I found a similar notebook and dataset 
in Kaggle (<a href="#moorei">Moore, 2021</a>).<br>
I looked for relationships between the prices of cars and their number of cylinders, horsepower, fuel efficiency etc., and produced scatter plots and a correlation matrix.<br>
The notebook I created is
<a href="https://github.com/ButterworthC/ml-course/blob/main/notebooks/Unit%202%20Kaggle%20EDA.ipynb">Unit 2 Kaggle EDA.ipynb</a>
and the dataset is <a href="https://github.com/ButterworthC/ml-course/blob/main/notebooks/data/kaggle_car_data_for_eda.csv">kaggle_car_data_for_eda.csv</a>
</p>

<h3>Unit 3: Correlation and Regression</h3>
<p>
We were directed to look at four notebooks (<a href="#unit3">University of Essex, 2024a</a>) 
and see how the calculated correlation and regression depend on the supplied data.
</p>
<p>
<b>Ex 1: Covariance and Pearson's correlation between two variables</b><br>
The first notebook was ready to run and immediately displayed a scatter plot between two sets of random numbers with weightings applied.
I quickly discovered that the number (1000) in the randn() function needed to be the same for both sets (of course), but that varying the 
factors (20, 10) and the offsets (100, 50) changed not only the shape of the plot but also the means and standard deviations for either 
dataset and the covariance and Pearson's correlation,
a.k.a. the correlation coefficient (<a href="#nield">Nield, 2022</a>: 171-179).
</p>
<p>
<b>Ex 2: Linear Regression</b><br>
The scipy library was imported as "stats" but referenced in the calls to the linregress() and pearsonr() methods as "stat" - preventing 
the notebook from running.
It occurred to me that this might have been a test to see if we were actually running these notebooks instead of just looking at them.
Passing arrays of x and y values to the SciPy lingress() method and receiving them as a tuple to variables for slope and intercept
(discarding the ones for r, p and std_err), the notebook then uses the SciPy pearsonr() method to calculate the correlation, before defining
its own simple function, based on y = mx + b (or y = mx + c as we say over here),
for predicting y based on a given x (<a href="#harrison">Harrison, 2019</a>: 191-222).
</p>
<p>
<b>Ex 3: Multiple Linear Regression</b><br>
This notebook uses the LinearRegression method (<a href="#nield">Nield, 2022</a>: 191) of the sklearn library instead of SciPy.  The regression has two independent variables, weight in kg and engine 
volume in cm<sup>3</sup>, and one dependent variable, CO2, in units of grams (per what? mile, km, hour? It doesn't say!).  
The regression object returns an array of two coefficients for the two regressions.
Its predict() method performs the same task as the custom function in the previous exercise.
</p>
<p>
<b>Ex 4: Polynomial Regression</b><br>
"A quadratic regression between the response Y and the predictor X would take the form: 
<i>Y = b<sub>0</sub> + b<sub>1</sub>X + b<sub>2</sub>X<sup>2</sup> + e</i>"
(<a href="#bruce">Bruce et al., 2020</a>: 188-189). 
The numpy library has a function called polyfit(), which takes arrays containing the x and y coordinates as its first two parameters, 
and a function called poly1d(), which takes the output from polyfit(), for creating a model based on the data.  
The value of r-squared given in this example is over 0.94, indicating a strong positive relation.
</p>

<h3>Unit 4: Linear Regression with Scikit-Learn</h3>
<p>
I had a look at the Fuel Consumption notebook (<a href="#unit4">University of Essex, 2024b</a>), 
which was very similar to the one I found in Kaggle in Unit 2. 
The first part included a correlation matrix, which included a redundant column "MODELYEAR" containing lots of NaN values.
I thought this could have been deleted to reduce the dimensions of the matrix and avoid confusion.
The seaborn pairplot() method was then used to plot regression matrices, from which the strong relation between 
fuel consumption and CO2 emissions is immediately apparent, along with the relation between city and highway MPGs, a trivial result.
In the second part of the notebook, the dataframe is split into training and testing datasets (80/20).
Then, the notebook imports the linear_model module from the scikit-learn library (<a href="#scavetta">Scavetta &amp; Angelov, 2021</a>)
and instantiates the LinearRegression class as regr,
using its fit() method with one-dimensional arrays of the x and y training data (engine size, CO2 emissions).
Coefficient and intercept are then given by regr.coeff_ and regr.intercept_.
Using matplotlib.pyplot, aliased as plt, the regression line is superimposed on the scatter plot.
The Mean absolute error, Residual sum of squares (MSE), and R2-score (R squared) are calculated.
This notebook finishes with a non-linear regression involving a custom sigmoid function.
</p>
<p>
Then I created a notebook to perform the correlation and regression tasks using the population and GDP datasets.
The end result needs a bit of tweaking because the mean populations do not look right (total world population of 67 billion!).
However, I managed to clean up a lot of the data, including deduplicating Aruba and sorting the population dataset so I could 
get both datasets synchronized (I almost got the zip() function to work).  I might come back to this one but for now I've got to move on.
</p>

<h3>Unit 5: Clustering</h3>
<p>
I went through the lecturecast and found some new things like SMC and Jaccard coefficient, as well as cohesion and separation, 
which will be very useful concepts.
</p>
<p>
The K-Clustering animations (<a href="#shabalin">Shabalin, N.D.</a>, <a href="#harris">Harris, 2014</a>) were very helpful in improving my understanding of this iterative process.
</p>
<p>
I created a notebook called
<a href="https://github.com/ButterworthC/ml-course/blob/main/notebooks/Unit05%20Jaccard%20coefficients.ipynb">Unit05 Jaccard coefficients.iypnb</a>
but did not find the hints useful, nor indeed ChatGPT, so I found an article online (<a href="#geeks2023a">Geeks for Geeks, 2023a</a>) which explained
that the Jaccard coefficient (a.k.a. Jaccard similarity or Jaccard index) is the size of the intersection of two sets divided by the size of their union.
The Python code for this follows on with ease because of the intersection() and union() set methods, and of course the len() function.
[It doesn't work yet!!!]
</p>

<h3>Unit 6: Clustering with Python</h3>
<p>
I found that the required reading, Chapter 6, was not relevant to this unit, so I read Chapter 15 instead and found out a lot about k-means clustering 
(<a href="#kubat">Kubat, 2021</a>: 297-310) although the required textbook does not mention Jaccard in the index.
</p>
<p>
I ran the k-means demo notebook and kept encountering a warning about a memory leak in Windows, which could be fixed "by setting the environment variable OMP_NUM_THREADS=4." (Python).
I did this but the warning persisted when I ran the notebook again.  However, the scatter plot of income vs age displayed, although the one of education, age and income did not.
I saw how the code looped through k values from 1 to 9, plotting the squared errors and producing an elbow at about k=3. 
The plot for the silhouette coefficient looped from k values of 2 to 9, because there need to be at least 2 clusters because it compares
the distance between clusters with the size of clusters and gives a value from -1 to 1. 
"1 indicates tight clusters, and 0 means overlapping clusters" (<a href="#harrison">Harrison, 2019</a>: 277).
</p>
<p>
I created a notebook called
<a href="https://github.com/ButterworthC/ml-course/blob/main/notebooks/Unit06%20K-Means%20clustering%20on%20iris%20data.ipynb">Unit06 K-Means clustering on iris data.ipynb</a>
and adapted some code for plotting clusters found in a Kaggle notebook (<a href="#khotijah">Khotijah, 2000</a>).
I was able to produce three distinct clusters and plot them in both 2D and 3D.
</p>
<p>
I found an article on the wine dataset (<a href="#geeks2024">Geeks for Geeks, 2024</a>) and created my own notebook called
<a href="https://github.com/ButterworthC/ml-course/blob/main/notebooks/Unit06%20K-Means%20clustering%20on%20wine%20data.ipynb">Unit06 K-Means clustering on wine data.ipynb</a>.
I plotted the first 12 possible combinations of alcohol with the other 12 attributes.
The first one to show 3 distinct clusters was, of course, the 12th of these: alcohol vs proline.
I had a lot of help from ChatGPT but when it came to plotting the centroids all I got was a grid with the wrong scales.
[I'll come back and do the third one, WeatherAUS, if I have time.]
</p>

<h3>Unit 7: Introduction to Artificial Neural Networks (ANNs)</h3>
<p>
This time, Chapter 6 was relevant, being titled "Artificial Neural Networks" (<a href="#kubat">Kubat, 2021</a>: 117-141).
I was already familiar with the sigmoid function and its shape but it would have been nice to see the shape of its first derivative, 
and perhaps its derivation. I found both of these online; the shape is a bell, which could have been deduced from the 
accelerating/inflection/decelerating shape of sigma (<a href="#geeks2023a">Geeks for Geeks, 2023a</a>).
</p>
<p>
The two key sentences in this chapter are "while there is no communication between neurons of the same layer, adjacent layers are fully interconnected.
Importantly, each neuron-to-neuron link is associated with a <i>weight</i>" (<a href="#kubat">Kubat, 2021</a>: 119). 
These concepts are illustrated with diagrams and there is an example with input and output values.
Although the text does not do a very good job of explaining "target vector," 
the explanation of MSE (Mean Squared Error) as a measure of "wrongness" is useful, and is made clear by reference to the example 
(<a href="#kubat">Kubat, 2021</a>: 122).
</p>
<p>
Backpropagation is something I've heard about but it turns out that what gets propagated backwards is the error.  
This is used to adjust the weightings of the connections between the layers of neurons until the MSE is minimised.
One problem with this chapter is that it does not explain what the superscripts mean until page 133, 
whereas I think page 117 would have been a better place - it would have saved me a lot of guess-work.
I was able to follow the explanations of ANN in this chapter until it introduced radial basis function networks on page 137.
I think I will have to come back to this after the module is finished.
</p>
<p>
The lecturecast described the same concepts, with a better introduction to the biological basis of neural networks.
We were directed to examine three notebooks (<a href="#unit7">University of Essex, 2024c</a>) containing examples of perceptrons.
</p>
<p>
In Ex 1 I was surprised to find that images can be put into notebook cells.
Also new to me, I think, was the fact that the type of an array declared as np.array is numpy.ndarray.
The simple perceptron example was quite easy to follow.
</p>
<p>
Ex 2 was also easy to follow, and the key to it seems to be:<br>
<span style="font-family: Courier, monospace;">weights[j] = weights[j] + (learning_rate * inputs[i][j] * error)</span>
</p>
<p>
Ex 3 illustrated very well the modification of the weight arrays. Many cells were used to demonstrate the principles, 
but in the last few cells the process was started again from scratch in a more succinct way.
</p>

<h3>Unit 8: Training an Artificial Neural Network</h3>
<p>
The lecturecast includes a link to a simple demonstration of a 2-layer MLP: <a href="https://hmkcode.com/netflow/">hmkcode.com/netflow/</a>, 
unfortunately without attribution or explanation. The main theme was backpropagation but much of the material was dealt with in Unit 7.


</p>


<a href=""></a><br>
<a href=""></a><br>
<a href=""></a><br>

<h3 id="refs">References</h3>

<p id="bruce">Bruce P, Bruce, A. &amp; Gedeck, P. (2019) <i>Practical Statistics for Data Scientists</i>. 2nd ed. Sebastopol, CA: O'Reilly Media Inc.</p>
<p id="geeks2023a">Geeks for Geeks (2023a) How to Calculate Jaccard Similarity in Python. Available from: <a href="https://www.geeksforgeeks.org/how-to-calculate-jaccard-similarity-in-python/">www.geeksforgeeks.org/how-to-calculate-jaccard-similarity-in-python/</a> [Accessed 10 May 2024]</p>
<p id="geeks2023b">Geeks for Geeks (2023b) Derivative of the Sigmoid Function. Available from: <a href="https://www.geeksforgeeks.org/derivative-of-the-sigmoid-function/">www.geeksforgeeks.org/derivative-of-the-sigmoid-function</a> [Accessed 12 May 2024]</p>
<p id="geeks2024">Geeks for Geeks (2024) Wine Dataset in Sklearn. Available from <a href="https://www.geeksforgeeks.org/wine-dataset/">www.geeksforgeeks.org/wine-dataset/</a> [Accessed 11 May 2024]</p>
<p id="harris">Harris, N. (2014) Visualizing K-Means Clustering. Available from: <a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">www.naftaliharris.com/blog/visualizing-k-means-clustering/</a> [Accessed 10 May 2024]</p>
<p id="harrison">Harrison, M. (2019) <i>Machine Learning Pocket Reference</i>. 1st ed. Sebastopol, CA: O'Reilly Media Inc.</p>
<p id="khotijah">Khotijah, S. (2000) K-Means Clustering of Iris Dataset. Available from: <a href="https://www.kaggle.com/code/khotijahs1/k-means-clustering-of-iris-dataset">www.kaggle.com/code/khotijahs1/k-means-clustering-of-iris-dataset</a> [Accessed 11 May 2024]</p>
<p id="kubat">Kubat, M. (2021) <i>An Introduction to Machine Learning</i>. 3rd ed. Cham: Springer</p>
<p id="moorei">Moore, I. (2021) Intro to Exploratory data analysis (EDA) in Python. Available from: <a href="https://www.kaggle.com/code/imoore/intro-to-exploratory-data-analysis-eda-in-python/notebook">www.kaggle.com/code/imoore/intro-to-exploratory-data-analysis-eda-in-python/notebook</a> [Accessed 26 April 2024]</p>
<p id="nield">Nield, T. (2022) <i>Essential Math for Data Science</i>. 1st ed. Sebastopol, CA: O'Reilly Media Inc.</p>
<p id="scavetta">Scavetta, R.J. &amp; Angelov, B (2021) <i>Python and R for the Modern Data Scientist</i>. 1st ed. Sebastopol, CA: O'Reilly Media Inc.</p>
<p id="schwab">Schwab, K. &amp; Zahid, S. (2020) The Future of Jobs. Available from: <a href="https://www3.weforum.org/docs/WEF_Future_of_Jobs_2020.pdf">www3.weforum.org/docs/WEF_Future_of_Jobs_2020.pdf</a> [Accessed 12 March 2024]</p>
<p id="shabalin">Shabalin, A.A. (N.D.) K-means clustering. Available from: <a href="http://shabal.in/visuals/kmeans/2.html">shabal.in/visuals/kmeans/2.html</a> [Accessed 10 May 2024]</p>
<p id="unit3">University of Essex. (2024a) Unit 3 of Machine Learning Module. Available from: <a href="https://www.my-course.co.uk/mod/page/view.php?id=958879">www.my-course.co.uk/mod/page/view.php?id=958879</a> [Accessed 8 May 2024]</p>
<p id="unit4">University of Essex. (2024b) Unit 4 of Machine Learning Module. Available from: <a href="https://www.my-course.co.uk/mod/book/view.php?id=958831&chapterid=12818">www.my-course.co.uk/mod/book/view.php?id=958831&chapterid=12818</a> [Accessed 9 May 2024]</p>
<p id="unit7">University of Essex. (2024c) Unit 7 of Machine Learning Module. Available from: <a href="https://www.my-course.co.uk/mod/page/view.php?id=958908">www.my-course.co.uk/mod/page/view.php?id=958908</a> [Accessed 13 May 2024]</p>
</body>
</html>